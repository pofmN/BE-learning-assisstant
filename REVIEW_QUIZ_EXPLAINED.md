# Final Review Quiz System - Complete Explanation

## ğŸ¯ **What Does This Module Do?**

When a student finishes all quizzes in a course, they can take a **final review quiz** that:

1. Analyzes which topics they struggled with (weak), did okay on (medium), or mastered (strong)
2. Generates 30 NEW quiz questions using AI (not reusing old questions)
3. Tracks their progress and can be resumed if interrupted
4. Compares performance with original attempts and gives personalized recommendations

---

## ğŸ“ **File Structure**

```
app/
â”œâ”€â”€ core/agents/review/                    # Brain of the review system
â”‚   â”œâ”€â”€ __init__.py                        # Exports all classes
â”‚   â”œâ”€â”€ eligibility_checker.py             # âœ… Checks if user can take review
â”‚   â”œâ”€â”€ quiz_selector.py                   # ğŸ¯ Picks questions based on performance
â”‚   â”œâ”€â”€ quiz_generator.py                  # ğŸ¤– Uses LLM to create new questions
â”‚   â”œâ”€â”€ performance_analyzer.py            # ğŸ“Š Compares old vs new performance
â”‚   â”œâ”€â”€ recommendation_generator.py        # ğŸ’¡ Generates study advice
â”‚   â””â”€â”€ prompts.py                         # ğŸ“ LLM prompts
â”‚
â”œâ”€â”€ api/v1/
â”‚   â”œâ”€â”€ review_quiz.py                     # ğŸŒ API endpoints for review quiz
â”‚   â””â”€â”€ quiz.py                            # ğŸŒ Quiz taking endpoints (updated)
â”‚
â””â”€â”€ models/
    â”œâ”€â”€ quiz_attempt.py                    # ğŸ’¾ Database: sessions & attempts (updated)
    â””â”€â”€ review_analysis.py                 # ğŸ’¾ Database: analysis results

```

---

## ğŸ”„ **Complete Flow (Step by Step)**

### **Phase 1: Check Eligibility**

```
User â†’ GET /courses/{id}/final-review/eligibility
         â†“
    EligibilityChecker.check_eligibility()
         â†“
    Query: Did user complete ALL quizzes in course?
         â†“
    If YES â†’ Return eligible=true
    If NO  â†’ Return eligible=false, missing_quizzes=[...]
    If has incomplete review â†’ Return session info to resume
```

**Code Location:** `app/core/agents/review/eligibility_checker.py`

**What it checks:**

- Count total quizzes in course
- Count how many the user attempted
- Look for unfinished review sessions
- Return eligibility status

---

### **Phase 2: Generate Review Quiz**

```
User â†’ POST /courses/{id}/final-review/generate
         â†“
    1. QuizSelector.select_questions_for_generation()
       â”‚
       â”œâ”€ Get all user's quiz attempts
       â”œâ”€ Calculate accuracy for each question
       â”œâ”€ Categorize: weak (<60%), medium (60-80%), strong (>80%)
       â”œâ”€ Select 30 questions based on strategy:
       â”‚   â€¢ balanced: 40% weak, 40% medium, 20% strong
       â”‚   â€¢ weak_focus: 70% weak, 30% other
       â”‚   â€¢ comprehensive: even distribution
       â””â”€ Return quiz data as examples
         â†“
    2. QuizGenerator.generate_questions()
       â”‚
       â”œâ”€ Format 10 example quizzes for prompt
       â”œâ”€ Call OpenAI GPT-4o-mini:
       â”‚   "Create 30 NEW questions like these examples..."
       â”œâ”€ Parse JSON response
       â”œâ”€ Validate question structure
       â””â”€ Return 30 new generated questions
         â†“
    3. Create QuizSession
       â”‚
       â”œâ”€ session_type = "final_review"
       â”œâ”€ generated_questions = JSON.dumps(30 questions)
       â”œâ”€ total_questions = 30
       â””â”€ status = "in_progress"
         â†“
    Return session_id to frontend
```

**Key Files:**

- `app/core/agents/review/quiz_selector.py` - Performance analysis & selection
- `app/core/agents/review/quiz_generator.py` - LLM generation
- `app/api/v1/review_quiz.py` - API endpoint

---

### **Phase 3: Take the Quiz**

```
User â†’ GET /sessions/{session_id}/questions
         â†“
    Check if session.generated_questions exists
         â†“
    Parse JSON â†’ 30 questions
         â†“
    Remove correct answers (don't show to user!)
         â†“
    Return questions with is_generated=true flag
```

```
User â†’ POST /sessions/{session_id}/submit
         â†“
    Check if is_generated=true
         â†“
    If generated:
       Use quiz index to find question in session.generated_questions
       Grade with _grade_generated_answer()
    Else:
       Normal quiz grading
         â†“
    Create QuizAttempt record
         â†“
    Update session stats
         â†“
    Return is_correct + explanation
```

**Code Location:** `app/api/v1/quiz.py` (updated `get_session_questions` and `submit_quiz_answer`)

---

### **Phase 4: Complete & Analyze**

```
User â†’ POST /sessions/{session_id}/complete
         â†“
    Update session status = "completed"
         â†“
    Calculate score_percentage
         â†“
    If session_type == "final_review":
       Trigger generate_review_analysis()
         â†“
       1. PerformanceAnalyzer.analyze_performance()
          â”‚
          â”œâ”€ Get original attempts (from regular quizzes)
          â”œâ”€ Get review attempts (from final review)
          â”œâ”€ Compare each question:
          â”‚   â€¢ Improved: was wrong â†’ now correct
          â”‚   â€¢ Regressed: was correct â†’ now wrong
          â”‚   â€¢ Persistent weak: still wrong
          â”‚   â€¢ Consistent strong: still correct
          â””â”€ Group by topic/section
            â†“
       2. RecommendationGenerator.generate_recommendations()
          â”‚
          â”œâ”€ Call LLM with performance data
          â”œâ”€ Get personalized study advice
          â””â”€ Include: grade, next steps, weak topics
            â†“
       3. Save ReviewQuizAnalysis to database
          â”‚
          â”œâ”€ original_avg_score
          â”œâ”€ review_score
          â”œâ”€ improvement_percentage
          â”œâ”€ question breakdown counts
          â”œâ”€ topic_breakdown (JSON)
          â””â”€ recommendations (JSON)
```

**Code Location:**

- `app/api/v1/quiz.py` - Triggers analysis
- `app/api/v1/review_quiz.py` - `generate_review_analysis()` function
- `app/core/agents/review/performance_analyzer.py`
- `app/core/agents/review/recommendation_generator.py`

---

### **Phase 5: View Insights**

```
User â†’ GET /courses/{id}/final-review/insights
         â†“
    Query ReviewQuizAnalysis table
         â†“
    Get most recent analysis for user + course
         â†“
    Parse JSON fields:
       â€¢ topic_breakdown
       â€¢ recommendations
       â€¢ insights
         â†“
    Format into ReviewInsightsResponse:
       â€¢ Performance summary (scores, improvement)
       â€¢ Question breakdown (improved, regressed, etc.)
       â€¢ Topic analysis (per section performance)
       â€¢ Recommendations (study advice)
       â€¢ Next steps (weak topics, study time, confidence)
         â†“
    Return to frontend
```

**Code Location:** `app/api/v1/review_quiz.py`

---

## ğŸ§  **Key Classes Explained**

### **1. EligibilityChecker**

```python
# Located: app/core/agents/review/eligibility_checker.py

class EligibilityChecker:
    def check_eligibility(user_id, course_id):
        """
        PURPOSE: Make sure user completed all quizzes before taking review

        LOGIC:
        1. Count total quizzes in course
        2. Count unique quizzes user attempted
        3. Check if there's an incomplete review session

        RETURNS:
        - eligible: bool (can take review?)
        - message: str (reason)
        - completed_quizzes: int
        - total_quizzes: int
        - existing_review: dict or None (if has incomplete session)
        """
```

---

### **2. QuizSelector**

```python
# Located: app/core/agents/review/quiz_selector.py

class QuizSelector:
    def select_questions_for_generation(user_id, course_id, strategy, count):
        """
        PURPOSE: Pick 30 questions based on how well user did

        LOGIC:
        1. Get all quiz attempts for this user/course
        2. Calculate accuracy for each quiz (correct/total * 100)
        3. Categorize:
           - weak: < 60% accuracy
           - medium: 60-80% accuracy
           - strong: > 80% accuracy
        4. Select based on strategy:
           - balanced: 12 weak + 12 medium + 6 strong
           - weak_focus: 21 weak + 9 other
           - comprehensive: mix of all
        5. Fetch full quiz data from database

        RETURNS:
        - quiz_data: List[Dict] (30 quizzes with all details)
        - distribution: Dict (counts per category)

        EXAMPLE:
        quiz_data = [
            {
                "question": "What is Python?",
                "question_type": "multiple_choice",
                "question_data": {...},
                "difficulty": "easy",
                "explanation": "..."
            },
            ... (29 more)
        ]

        distribution = {
            "weak_topics": 12,
            "medium_topics": 12,
            "strong_topics": 6
        }
        """
```

---

### **3. QuizGenerator**

```python
# Located: app/core/agents/review/quiz_generator.py

class QuizGenerator:
    def generate_questions(example_quizzes, count=30):
        """
        PURPOSE: Use AI to create NEW questions based on examples

        LOGIC:
        1. Take first 10 examples (to save tokens)
        2. Format them nicely for the prompt
        3. Call OpenAI API:
           System: "You're a quiz creator"
           User: "Create 30 new questions like these..."
        4. Parse JSON response
        5. Validate each question has required fields
        6. Return list of new questions

        PROMPT EXAMPLE:
        "Create 30 NEW quiz questions based on these examples:

        1. Question: What is Python?
           Type: multiple_choice
           Difficulty: easy

        2. Question: Explain loops...
           Type: short_answer
           Difficulty: medium

        Return JSON array with same structure."

        LLM RESPONSE:
        [
            {
                "question": "What is a variable in Python?",
                "question_type": "multiple_choice",
                "question_data": {
                    "options": [...],
                    "correct_answer": "option_a"
                },
                "difficulty": "easy",
                "explanation": "..."
            },
            ... (29 more)
        ]

        FALLBACK: If LLM fails, return modified versions of examples
        """
```

---

### **4. PerformanceAnalyzer**

```python
# Located: app/core/agents/review/performance_analyzer.py

class PerformanceAnalyzer:
    def analyze_performance(user_id, course_id, review_session_id):
        """
        PURPOSE: Compare how user did on review vs original attempts

        LOGIC:
        1. Get original attempts (from regular quizzes)
        2. Get review attempts (from final review - generated questions)
        3. For each question in review:
           - Was it in original attempts?
           - If yes, compare: was_correct vs is_correct_now
           - Categorize:
             * improved: was wrong â†’ now correct âœ…
             * regressed: was correct â†’ now wrong âŒ
             * persistent_weak: still wrong ğŸ˜Ÿ
             * consistent_strong: still correct ğŸ’ª
        4. Group by section/topic
        5. Calculate improvement percentages

        RETURNS: PerformanceReport with:
        - improved_questions: [quiz_ids...]
        - regressed_questions: [quiz_ids...]
        - persistent_weaknesses: [quiz_ids...]
        - consistent_strengths: [quiz_ids...]
        - topic_analysis: {
            "Section 1": {
                "original_score": 65.0,
                "review_score": 80.0,
                "improvement": +15.0
            },
            ...
          }
        """
```

---

### **5. RecommendationGenerator**

```python
# Located: app/core/agents/review/recommendation_generator.py

class RecommendationGenerator:
    def generate_recommendations(course_id, performance_report, original_score, review_score):
        """
        PURPOSE: Use AI to generate personalized study advice

        LOGIC:
        1. Format performance data for prompt
        2. Call OpenAI with structured output
        3. Request:
           - 3-5 prioritized recommendations
           - Weak topics to focus on
           - Suggested study time
           - Review timeline
           - Confidence level assessment

        PROMPT EXAMPLE:
        "Student performance:
        - Original: 68%
        - Review: 75%
        - Improved: 5 questions
        - Still struggling: 8 questions

        Topics with issues:
        - Loops (40% accuracy)
        - Functions (55% accuracy)

        Generate study recommendations..."

        LLM RESPONSE:
        {
            "recommendations": [
                {
                    "priority": "high",
                    "topic": "Loops",
                    "suggestion": "Practice writing for and while loops...",
                    "reason": "Still making mistakes in 8/10 loop questions",
                    "study_resources": ["Chapter 4", "Practice exercises"]
                },
                ...
            ],
            "next_steps": {
                "weak_topics": ["Loops", "Functions"],
                "suggested_study_time": "3-4 hours",
                "review_again_after": "5 days",
                "confidence_level": "medium"
            },
            "motivation_message": "You've improved by 7%! Keep practicing..."
        }
        """
```

---

## ğŸ’¾ **Database Schema**

### **QuizSession** (updated)

```sql
CREATE TABLE quiz_sessions (
    id SERIAL PRIMARY KEY,
    user_id INT NOT NULL,
    course_id INT NOT NULL,
    section_id INT,  -- NULL for final review

    -- NEW FIELDS for review quiz:
    session_type VARCHAR(50) DEFAULT 'regular',  -- 'regular', 'section', 'final_review'
    generated_questions TEXT,  -- JSON array of LLM-generated questions

    status VARCHAR DEFAULT 'in_progress',
    total_questions INT,
    correct_answers INT,
    score_percentage FLOAT,
    started_at TIMESTAMP,
    completed_at TIMESTAMP
);
```

### **ReviewQuizAnalysis** (new table)

```sql
CREATE TABLE review_quiz_analysis (
    id SERIAL PRIMARY KEY,
    user_id INT NOT NULL,
    course_id INT NOT NULL,
    review_session_id INT NOT NULL,  -- Links to quiz_sessions

    -- Performance metrics
    total_original_attempts INT,
    original_avg_score FLOAT,
    review_score FLOAT,
    improvement_percentage FLOAT,

    -- Question breakdown
    improved_count INT,
    regressed_count INT,
    persistent_weak_count INT,
    consistent_strong_count INT,

    -- JSON data
    topic_breakdown TEXT,  -- JSON: [{section, scores, improvement}, ...]
    recommendations TEXT,  -- JSON: LLM recommendations
    insights TEXT,  -- JSON: {grade, next_steps, motivation}

    analysis_generated_at TIMESTAMP
);
```

---

## ğŸ”Œ **API Endpoints**

### **1. Check Eligibility**

```http
GET /api/v1/courses/{course_id}/final-review/eligibility

Response:
{
    "eligible": true,
    "message": "You have completed all quizzes. Ready for final review!",
    "completed_quizzes": 45,
    "total_quizzes": 45,
    "existing_review": null  // or {session_id, total, answered} if resuming
}
```

### **2. Generate Review Quiz**

```http
POST /api/v1/courses/{course_id}/final-review/generate
Body: {
    "strategy": "balanced",  // or "weak_focus", "comprehensive"
    "question_count": 30
}

Response:
{
    "session_id": 123,
    "total_questions": 30,
    "selection_strategy": "balanced",
    "question_distribution": {
        "weak_topics": 12,
        "medium_topics": 12,
        "strong_topics": 6
    },
    "message": "Final review quiz generated successfully. Start when ready!"
}
```

### **3. Get Questions**

```http
GET /api/v1/sessions/123/questions

Response: [
    {
        "quiz_id": 0,  // Index for generated questions
        "question": "What is the purpose of a loop in programming?",
        "question_type": "multiple_choice",
        "question_data": {
            "options": [
                {"id": "option_a", "text": "To repeat code"},
                {"id": "option_b", "text": "To store data"},
                {"id": "option_c", "text": "To define functions"},
                {"id": "option_d", "text": "To import modules"}
            ]
            // NO correct_answer - removed for user
        },
        "difficulty": "medium",
        "is_generated": true  // Flag indicating LLM-generated
    },
    ... (29 more)
]
```

### **4. Submit Answer**

```http
POST /api/v1/sessions/123/submit
Body: {
    "quiz_id": 0,
    "user_answer": {"selected_id": "option_a"},
    "time_spent": 30
}

Response: {
    "attempt_id": 456,
    "quiz_id": 0,
    "is_correct": true,
    "user_answer": {"selected_id": "option_a"},
    "correct_answer": {"options": [...], "correct_answer": "option_a"},
    "explanation": "Loops are used to repeat code multiple times...",
    "question": "What is the purpose of a loop...",
    "question_type": "multiple_choice"
}
```

### **5. Complete Quiz**

```http
POST /api/v1/sessions/123/complete

Response: {
    "session_id": 123,
    "total_questions": 30,
    "correct_answers": 22,
    "incorrect_answers": 8,
    "score_percentage": 73.33,
    "completed_at": "2025-12-24T10:30:00Z",
    "attempts": [...]
}

// Automatically triggers analysis in background
```

### **6. Get Insights**

```http
GET /api/v1/courses/{course_id}/final-review/insights

Response: {
    "analysis_id": 789,
    "review_session_id": 123,
    "completion_date": "2025-12-24T10:30:00Z",
    "performance_summary": {
        "original_avg_score": 68.5,
        "review_score": 73.33,
        "improvement": 4.83,
        "grade": "C+"
    },
    "question_breakdown": {
        "improved": 5,
        "regressed": 2,
        "persistent_weak": 3,
        "consistent_strong": 20
    },
    "topic_analysis": [
        {
            "section": "Loops",
            "section_id": 5,
            "original_score": 55.0,
            "review_score": 65.0,
            "improvement": 10.0,
            "status": "improving"
        },
        ...
    ],
    "recommendations": [
        {
            "priority": "high",
            "topic": "Loops",
            "suggestion": "Practice nested loops and loop control...",
            "reason": "Still making errors in loop questions",
            "study_resources": ["Chapter 4", "Exercises 1-10"]
        },
        ...
    ],
    "next_steps": {
        "weak_topics": ["Loops", "Recursion"],
        "suggested_study_time": "3-4 hours",
        "review_again_after": "5 days",
        "confidence_level": "medium"
    }
}
```

---

## ğŸ¨ **Frontend Integration Guide**

### **Complete User Journey**

```javascript
// STEP 1: Check if user can take review
async function checkEligibility(courseId) {
  const response = await fetch(
    `/api/v1/courses/${courseId}/final-review/eligibility`
  );
  const data = await response.json();

  if (!data.eligible) {
    alert(data.message); // "You need to complete 5 more quizzes"
    return false;
  }

  if (data.existing_review) {
    // User has incomplete review
    if (
      confirm(
        `Resume existing review? (${data.existing_review.answered}/${data.existing_review.total} completed)`
      )
    ) {
      return data.existing_review.session_id;
    }
  }

  return true;
}

// STEP 2: Generate review quiz
async function generateReview(courseId, strategy = "balanced") {
  showLoadingSpinner("Generating personalized review quiz...");

  const response = await fetch(
    `/api/v1/courses/${courseId}/final-review/generate`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        strategy: strategy, // 'balanced', 'weak_focus', 'comprehensive'
        question_count: 30,
      }),
    }
  );

  const data = await response.json();
  hideLoadingSpinner();

  // Show distribution info
  console.log(`Generated ${data.total_questions} questions:`);
  console.log(`- ${data.question_distribution.weak_topics} from weak topics`);
  console.log(
    `- ${data.question_distribution.medium_topics} from medium topics`
  );
  console.log(
    `- ${data.question_distribution.strong_topics} from strong topics`
  );

  return data.session_id;
}

// STEP 3: Load and display questions
async function loadQuestions(sessionId) {
  const response = await fetch(`/api/v1/sessions/${sessionId}/questions`);
  const questions = await response.json();

  // Render questions
  questions.forEach((q, index) => {
    renderQuestion(q, index);
  });
}

// STEP 4: Submit answers
async function submitAnswer(sessionId, quizId, userAnswer) {
  const response = await fetch(`/api/v1/sessions/${sessionId}/submit`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      quiz_id: quizId,
      user_answer: userAnswer,
      time_spent: 45,
    }),
  });

  const result = await response.json();

  // Show immediate feedback
  if (result.is_correct) {
    showCorrect(result.explanation);
  } else {
    showIncorrect(result.correct_answer, result.explanation);
  }
}

// STEP 5: Complete quiz
async function completeQuiz(sessionId) {
  const response = await fetch(`/api/v1/sessions/${sessionId}/complete`, {
    method: "POST",
  });

  const results = await response.json();

  showResults(results);

  // Wait a moment for analysis to generate
  setTimeout(() => loadInsights(courseId), 2000);
}

// STEP 6: Show insights and recommendations
async function loadInsights(courseId) {
  const response = await fetch(
    `/api/v1/courses/${courseId}/final-review/insights`
  );
  const insights = await response.json();

  // Display performance summary
  displayPerformanceSummary(insights.performance_summary);

  // Display topic breakdown with charts
  displayTopicAnalysis(insights.topic_analysis);

  // Display recommendations
  displayRecommendations(insights.recommendations);

  // Display next steps
  displayNextSteps(insights.next_steps);
}
```

---

## ğŸ› **Common Issues & Debugging**

### **Issue 1: "No quiz attempts found"**

```
ERROR: ValueError: No quiz attempts found for this user/course

REASON: User hasn't attempted any quizzes yet
FIX: Check eligibility first - endpoint will tell you what's missing
```

### **Issue 2: "Too few valid questions" from LLM**

```
ERROR: Only 12/30 questions validated

REASON: LLM returned malformed JSON or missing fields
FIX: Check _validate_question() - it has strict requirements
FALLBACK: System automatically uses fallback questions
```

### **Issue 3: Session already exists**

```
ERROR: eligibility.existing_review is not None

REASON: User started but didn't finish a review
FIX: Frontend should ask: "Resume or start new?"
      - Resume: Use existing session_id
      - New: Delete old session first (or let it expire)
```

### **Issue 4: Generated questions missing correct_answer**

```
ERROR: KeyError: 'correct_answer'

REASON: get_session_questions removes answers for user
FIX: This is intentional! Use submit endpoint to grade answers
      Answers are stored in session.generated_questions (backend only)
```

---

## ğŸ§ª **Testing Guide**

### **Manual Testing Flow**

```bash
# 1. Create a test user and course
# 2. Complete some quizzes (mix of correct/incorrect)

# 3. Check eligibility
curl http://localhost:8000/api/v1/courses/1/final-review/eligibility

# 4. Generate review
curl -X POST http://localhost:8000/api/v1/courses/1/final-review/generate \
  -H "Content-Type: application/json" \
  -d '{"strategy": "balanced", "question_count": 30}'

# 5. Get questions (should see is_generated: true)
curl http://localhost:8000/api/v1/sessions/123/questions

# 6. Submit answers
curl -X POST http://localhost:8000/api/v1/sessions/123/submit \
  -H "Content-Type: application/json" \
  -d '{"quiz_id": 0, "user_answer": {"selected_id": "option_a"}}'

# 7. Complete
curl -X POST http://localhost:8000/api/v1/sessions/123/complete

# 8. View insights
curl http://localhost:8000/api/v1/courses/1/final-review/insights
```

### **Unit Test Examples**

```python
# test_quiz_selector.py
def test_categorizes_questions_by_performance():
    selector = QuizSelector(db)
    quizzes, dist = selector.select_questions_for_generation(
        user_id=1, course_id=1, strategy='balanced'
    )

    assert len(quizzes) == 30
    assert dist['weak_topics'] == 12  # 40%
    assert dist['medium_topics'] == 12  # 40%
    assert dist['strong_topics'] == 6  # 20%

# test_quiz_generator.py
def test_generates_valid_questions():
    generator = QuizGenerator(db)
    examples = [mock_quiz() for _ in range(10)]

    questions = generator.generate_questions(examples, count=30)

    assert len(questions) == 30
    for q in questions:
        assert 'question' in q
        assert 'question_type' in q
        assert 'question_data' in q
        assert 'explanation' in q
```

---

## ğŸ’¡ **Key Takeaways**

1. **Simple Flow**: Select examples â†’ LLM generates new â†’ Store â†’ User takes â†’ Analyze â†’ Recommend

2. **Smart Selection**: Picks questions based on actual performance (weak/medium/strong)

3. **Cost Efficient**: Only sends 10 examples to LLM, not entire course content

4. **Resumable**: User can pause and continue later (sessions stored in DB)

5. **Insightful**: Compares performance, identifies patterns, gives personalized advice

6. **Modular**: Each component has single responsibility, easy to test/modify

---

## ğŸ“š **Further Reading**

- **LangChain Docs**: https://python.langchain.com/docs/
- **OpenAI API**: https://platform.openai.com/docs/
- **FastAPI**: https://fastapi.tiangolo.com/
- **SQLAlchemy**: https://docs.sqlalchemy.org/

---

**Need help with a specific part? Ask me about:**

- How eligibility checking works
- How quiz selection algorithm decides what to pick
- How LLM prompt is structured
- How grading works for generated questions
- How performance analysis compares attempts
- How to customize recommendation prompts
- How to add new strategies (beyond balanced/weak_focus/comprehensive)
